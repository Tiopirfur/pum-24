{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "383075d4f797ffc1",
   "metadata": {},
   "source": [
    "# Lab 4-1: Data preprocessing. Building a regression model.\n",
    "\n",
    "In this lab will write a simple regression model to predict the solubility of chemical molecules. We will use the `RDKit` library to calculate molecular descriptors from SMILES strings, and the `scikit-learn` library to build a regression model.\n",
    "\n",
    "We will learn how to get different molecular decriptors from SMILES strings with the use od `RDKit` library. Then we will preprocess data, implement a featurizer for automatic input data preparation, and wrap the whole process in a simple, user-friendly web app using `Streamlit`.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49a509e53faea6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>Solubility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N,N,N-trimethyloctadecan-1-aminium bromide</td>\n",
       "      <td>[Br-].CCCCCCCCCCCCCCCCCC[N+](C)(C)C</td>\n",
       "      <td>-3.616127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Benzo[cd]indol-2(1H)-one</td>\n",
       "      <td>O=C1Nc2cccc3cccc1c23</td>\n",
       "      <td>-3.254767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4-chlorobenzaldehyde</td>\n",
       "      <td>Clc1ccc(C=O)cc1</td>\n",
       "      <td>-2.177078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zinc bis[2-hydroxy-3,5-bis(1-phenylethyl)benzo...</td>\n",
       "      <td>[Zn++].CC(c1ccccc1)c2cc(C(C)c3ccccc3)c(O)c(c2)...</td>\n",
       "      <td>-3.924409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4-({4-[bis(oxiran-2-ylmethyl)amino]phenyl}meth...</td>\n",
       "      <td>C1OC1CN(CC2CO2)c3ccc(Cc4ccc(cc4)N(CC5CO5)CC6CO...</td>\n",
       "      <td>-4.662065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name  \\\n",
       "0         N,N,N-trimethyloctadecan-1-aminium bromide   \n",
       "1                           Benzo[cd]indol-2(1H)-one   \n",
       "2                               4-chlorobenzaldehyde   \n",
       "3  zinc bis[2-hydroxy-3,5-bis(1-phenylethyl)benzo...   \n",
       "4  4-({4-[bis(oxiran-2-ylmethyl)amino]phenyl}meth...   \n",
       "\n",
       "                                              SMILES  Solubility  \n",
       "0                [Br-].CCCCCCCCCCCCCCCCCC[N+](C)(C)C   -3.616127  \n",
       "1                               O=C1Nc2cccc3cccc1c23   -3.254767  \n",
       "2                                    Clc1ccc(C=O)cc1   -2.177078  \n",
       "3  [Zn++].CC(c1ccccc1)c2cc(C(C)c3ccccc3)c(O)c(c2)...   -3.924409  \n",
       "4  C1OC1CN(CC2CO2)c3ccc(Cc4ccc(cc4)N(CC5CO5)CC6CO...   -4.662065  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/aqsol.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab12a34250cf4e1",
   "metadata": {},
   "source": [
    "## Calculating molecular descriptors with RDKit\n",
    "\n",
    "**RDKit is a collection of cheminformatics tools, widely used to analyze and process chemical data.** We will use some of its functionalities to calculate molecular descriptors for the molecules in our dataset, and use the descriptors as input features for a regression model that will learn to predict the solubility of the molecules from SMILES strings. \n",
    "\n",
    "For this task will use only one submodule of RDKit, `rdkit.Chem.rdMolDescriptors`, which contains functions for calculating the molecular descriptors we are interested in. Feel free to explore RDKit yourself by reading [rdMolDescriptors documentation](https://www.rdkit.org/docs/source/rdkit.Chem.rdMolDescriptors.html) and [RDKit documentation](https://www.rdkit.org/docs/index.html).\n",
    "\n",
    "The most important RDKit class is `Mol` which represents a molecule with its atoms, bonds, spatial conformation, etc. Most RDKit functions, including those for calculating molecular descriptors, take a `Mol` object as input. If you have a SMILES string, you can create a `Mol` object using the `Chem.MolFromSmiles` function, as shown below:\n",
    "\n",
    "```python\n",
    "from rdkit import Chem\n",
    "\n",
    "smiles = 'C1C(=O)NC2=C(C=C(C=C2)[N+](=O)[O-])C(=N1)C3=CC=CC=C3'\n",
    "mol = Chem.MolFromSmiles(smiles)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d60f4277dd88387",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (2) does not match length of index (9980)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Now we can calculate the molecular descriptors\u001b[39;00m\n\u001b[0;32m     10\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmol_wt\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmol\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(rdMolDescriptors\u001b[38;5;241m.\u001b[39mCalcExactMolWt)             \u001b[38;5;66;03m# Molecular weight\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmol\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(rdMolDescriptors\u001b[38;5;241m.\u001b[39mCalcCrippenDescriptors)[\u001b[38;5;241m0\u001b[39m]    \u001b[38;5;66;03m# LogP (lipophilicity)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_heavy_atoms\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmol\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(rdMolDescriptors\u001b[38;5;241m.\u001b[39mCalcNumHeavyAtoms) \u001b[38;5;66;03m# Number of heavy atoms\u001b[39;00m\n\u001b[0;32m     13\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_HBD\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmol\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(rdMolDescriptors\u001b[38;5;241m.\u001b[39mCalcNumHBD)                \u001b[38;5;66;03m# Number of hydrogen bond donors\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pum\\lib\\site-packages\\pandas\\core\\frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 4311\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pum\\lib\\site-packages\\pandas\\core\\frame.py:4524\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4516\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4517\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4522\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4524\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4527\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4528\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m   4530\u001b[0m     ):\n\u001b[0;32m   4531\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pum\\lib\\site-packages\\pandas\\core\\frame.py:5266\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   5263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   5265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 5266\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5267\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   5268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5269\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[0;32m   5270\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5273\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[0;32m   5274\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pum\\lib\\site-packages\\pandas\\core\\common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    578\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (2) does not match length of index (9980)"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit import RDLogger  \n",
    "RDLogger.DisableLog('rdApp.*') # Disabling rdkit warnings\n",
    "\n",
    "# First, we will create a mol object for each molecule in the dataset, and store it in a new column\n",
    "df['mol'] = df['SMILES'].apply(Chem.MolFromSmiles)\n",
    "\n",
    "# Now we can calculate the molecular descriptors\n",
    "df['mol_wt'] = df['mol'].apply(rdMolDescriptors.CalcExactMolWt)             # Molecular weight\n",
    "df['logp'] = df['mol'].apply(rdMolDescriptors.CalcCrippenDescriptors)[0]    # LogP (lipophilicity)\n",
    "df['num_heavy_atoms'] = df['mol'].apply(rdMolDescriptors.CalcNumHeavyAtoms) # Number of heavy atoms\n",
    "df['num_HBD'] = df['mol'].apply(rdMolDescriptors.CalcNumHBD)                # Number of hydrogen bond donors\n",
    "df['num_HBA'] = df['mol'].apply(rdMolDescriptors.CalcNumHBA)                # Number of hydrogen bond acceptors\n",
    "df['aromatic_rings'] = df['mol'].apply(rdMolDescriptors.CalcNumAromaticRings) # Number of aromatic rings\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b96d1421ed6e46",
   "metadata": {},
   "source": [
    "## Exercise 1: Extract features and split the dataset (1 point)\n",
    "\n",
    "Now that we have the molecular descriptors, we can use them as input features for a regression model. As we did in the previous lab, we will extract the **input features** $X$ and the target variable $y$, and split the dataset into training and test sets.\n",
    "\n",
    "Let's use the newly calculated molecular descriptors as input features, and the solubility as the target variable.\n",
    "\n",
    "1. Extract the input features and the target variable from the dataset\n",
    "2. Split the dataset into training and test sets, with a test size of 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2da0c4db1756bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the relevant columns of the dataframe (features and target)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc72dac8094e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6e695e991ef6e8",
   "metadata": {},
   "source": [
    "## Exercise 2: Build a regression model (2 points)\n",
    "\n",
    "You already know the `scikit-learn` library, as we used it to build some classifier models in the previous labs. Now, we will use it to build a regression model. Linear regression is the simplest regression model, and it is a good starting point for regression problems. It is implemented in scikit-learn as `LinearRegression`. You should also try a more complex model, such as `SVR` (Support Vector Regression) and compare the results.\n",
    "\n",
    "1. Train a `LinearRegression` model on the training set. Report $RMSE$ (Root Mean Squared Error) and $R^2$ score on the train and test sets.\n",
    "2. Train an `SVR` model on the training set. Report $RMSE$ and $R^2$ score on the train and test sets. **Use the default hyperparameters for now.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd091b6b7bf3023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "...\n",
    "\n",
    "svr = SVR()\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba17e8dbfa3cfdc",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "\n",
    "**Standardization** of datasets is a common requirement for many machine learning estimators implemented in scikit-learn. If some features of our data have very different scales (for example, one feature is in the range $[0, 1]$ and another can potentially be any positive number), **some models** might consider the feature with larger numerical values to be more important. This can be a problem, as we want our model to be able to learn from all features equally.\n",
    "\n",
    "Standardization transform our data in such a way that its distribution will have a mean value $\\mu = 0$ and standard deviation $\\sigma = 1$. We can achieve this by using the `StandardScaler` from the `sklearn` library.\n",
    "\n",
    "Another common preprocessing step is **normalization**. In this case, the data is scaled to a fixed range, usually $[0, 1]$. The motivation to use this scaling method includes preserving zeros in sparse data. For example, when making predictions about the expected outcome of an anticancer therapy, the value of $0$ observed tumors in a patient is probably much more informative for the predictive model than any other number of observed tumors alone. We can scale data to a certain range by using the `MinMaxScaler` from the `sklearn` library.\n",
    "\n",
    "You can read more about those two feature scaling methods in the [Scikit-learn preprocessing docs](https://scikit-learn.org/stable/modules/preprocessing.html).\n",
    "\n",
    "<center>\n",
    "    <img src=\"imgs/logp-std.png\" width=\"500\">\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "**Remember that the scaler should be fitted only on the training set, and then used to transform both the training and test sets.** If we first transform the entire dataset and then split it into training and test sets, we are leaking the information about **test set data distribution** to the model, which would lead to overly optimistic results.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "...\n",
    "```\n",
    "\n",
    "\n",
    "## Exercise 3: Preprocess the input features (2 points)\n",
    "\n",
    "1. Standardize the input features using the `StandardScaler` from `sklearn`. **Fit the scaler on the training set and then transform both the training and test sets**.\n",
    "2. Train both linear regression and SVR models on the standardized training set. Report $RMSE$ and $R^2$ score on the train and test sets. How do the results compare to the previous models? Which model benefits from standardization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b760550de9e5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7788a1513ee0f54",
   "metadata": {},
   "source": [
    "## *Exercise: Featurizer class\n",
    "\n",
    "All the data manipulations, including the generation of features (molecular descriptors) and scaling we did completely by hand. Imagine that we have a new dataset with molecules for which we want to predict the solubility. We would have to repeat all the steps we did above, which is not at all user-friendly.\n",
    "\n",
    "Let's encapsulate all the steps in a single class called `Featurizer`. \n",
    "\n",
    "**Featurizer is a piece of code which takes care of extracting the features and preparing them** in such a way that they can be used as an input for a ML model. In this fashion, our featurizer will be able to take a list of SMILES strings and return a dataframe with the molecular descriptors, ready to be used in any regression model.\n",
    "\n",
    "The `Featurizer` class should have the following methods:\n",
    "\n",
    "1. `__init__(self, smiles)`\n",
    "\n",
    "    The constructor method should take an array of **training set** SMILES strings as input and calculate the molecular descriptors for each molecule (in a `pandas.DataFrame`, as we did earlier). **It should also fit a scaler to the descriptors.** The descriptors should be stored in a dataframe, and the scaler should be stored as an attribute of the class.\n",
    "      \n",
    "\n",
    "2. `featurize(self, smiles)`\n",
    "    \n",
    "   This method should take an array of SMILES strings as input and return a dataframe with the molecular descriptors calculated with RDKit. **The descriptors should be standardized using the scaler which was fitted to the training set in the `__init__` method**.\n",
    "\n",
    "3. **You are more than welcome to implement other methods** if you think it will make your code cleaner or more efficient. Maybe we could have a separate method for the calculation of molecular descriptors, or for the scaling of the features?\n",
    "\n",
    "\n",
    "### Here is an example of how our `Featurizer` class should be used\n",
    "```python\n",
    "featurizer = Featurizer(X_train['SMILES'])  # Create an instance of the Featurizer class, passing the training set SMILES strings\n",
    "\n",
    "# Say we want to predict the solubility of the following molecules\n",
    "some_SMILES = ['CC(=O)Oc1ccccc1C(=O)O',     \n",
    "               'C1=CC(=C(C=C1[C@H](CN)O)O)O',\n",
    "               'C1=NC2=C(C(=N1)N)N=CN2C3C(C(C(O3)CO)O)O']\n",
    "\n",
    "# Extract the features\n",
    "X = featurizer.featurize(some_SMILES)\n",
    "\n",
    "...\n",
    "\n",
    "y_pred = model.predict(X)   # Get predictions\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdba06d2dc54ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Featurizer:\n",
    "    def __init__(self, smiles):\n",
    "        ...\n",
    "    \n",
    "    def featurize(self, smiles):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8054c47c9dab6f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your featurizer\n",
    "\n",
    "featurizer = Featurizer(...) # Pass the training set SMILES strings to the constructor\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "tryptamines = pd.read_csv('data/tryptamines.csv')   # Load a new dataset\n",
    "tryptamines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb8e9765cba0f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = featurizer.featurize(tryptamines['SMILES'])   # Extract features for the new dataset\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c47cf735cf5cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the solubility of the new molecules\n",
    "\n",
    "y_pred = svr.predict(X)  # Use the linear regression model to predict the solubility\n",
    "\n",
    "tryptamines['Solubility'] = y_pred  # Add the predictions to the dataframe\n",
    "tryptamines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f535942fbf64684",
   "metadata": {},
   "source": [
    "## Cross-validation and grid search\n",
    "\n",
    "Now that we have a working regression model, we can try to improve its performance by tuning the hyperparameters. Last time we tuned the hyperparameters by hand, but as you may guess, that is not the most efficient way to do it. What we can (and will) do is employ a **search algorithm** to find the best hyperparameters automatically. This piece of code will try different hyperparameters and return the best combination.\n",
    "\n",
    "Last time we introduced the concept of a **validation set**, which is a subset of the training set used to evaluate the model's performance during hyperparameter tuning. To tune the hyperparameters of a classifier model we cut out a part of the training set and used the accuracy on this validation set as a metric to evaluate the model's performance.\n",
    "\n",
    "In practice, when tuning hyperparameters of a machine learning model, we do not usually create a single validation set. Instead, we employ a srategy called **cross-validation**. In $k$-fold cross-validation, the process of training and evaluating the model is repeated $k$ times, with each repetition using a different validation set. Thanks to this strategy, each data point present in the original training set will have the chance to be included in a validation sets. The metrics reported from $k$ folds are then averaged to get a more reliable estimate of the model's performance.\n",
    "\n",
    "<center>\n",
    "<img src=\"imgs/cross-validation.png\" width=\"800\">\n",
    "</center>\n",
    "\n",
    "Luckily, we do not have to implement this algorithm ourselves, as scikit-learn provides a `GridSearchCV` class that can help us with this task. It performs an exhaustive search over a specified parameter grid and evaluates the model's performance using $k$-fold cross-validation. By default, `GridSearchCV` uses a 5-fold cross-validation, which is a common choice for $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aeecb460602ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# We will define a grid of hyperparameters as a dictionary. The keys are the hyperparameter names, and the values are lists of possible values to try.\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [2, 4, 6, 8, 10],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    svr, # the model\n",
    "    param_grid, # the grid of hyperparameters\n",
    "    verbose=2 # print the progress\n",
    ")\n",
    "\n",
    "svr = grid_search.fit(X_train, y_train) # GridSearchCV.fit() returns the best model, and we can save it to a new variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc8f7c1b8fb2afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Report the best hyperparameters and RMSE on the testing set\n",
    "\n",
    "print('Best hyperparameters:', svr.get_params())\n",
    "\n",
    "y_pred = svr.predict(X_test)\n",
    "\n",
    "print('-'*50)\n",
    "print('Testing set rmse:', mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f84dd5deebbd0d9",
   "metadata": {},
   "source": [
    "### *Randomized search\n",
    "\n",
    "`RandomizedSearchCV` is an alternative to `GridSearchCV`. Instead of trying all possible combinations of fixed hyperparameters, it samples a fixed number of hyperparameter settings from specified probability distributions. Although you may be hesitant to use it, as it is not an exhaustive search, it is often more efficient than grid search. Randomized search is especially useful when we have many hyperparameters to tune, and we are not sure which ones are the most important. Take a look at the figure below and try to understand why this is the case.\n",
    "\n",
    "<center>\n",
    "<img src=\"imgs/grid-vs-random.png\", width=\"600\">\n",
    "</center>\n",
    "\n",
    "Randomized search is implemented in the same way as grid search, but with a different class. It also requires a dictionary of hyperparameters, but instead of specifying a list of values to try, we specify a **probability distribution**. See the example below:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "param_distributions = {\n",
    "    'some_discete_hyperparameter': np.arange(1, 10), # uniform dicrete distribution from 1 to 10\n",
    "    'some_continuous_hyperparameter': uniform(0, 1), # uniform continuous distribution from 0 to 1\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    model,\n",
    "    param_distributions,\n",
    "    n_iter=100, # number of random samples to try\n",
    "\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc86bf1e4ca2c1d0",
   "metadata": {},
   "source": [
    "## Saving and loading Python objects\n",
    "\n",
    "Both the featurizer and the trained regression model can be saved to disk as Python objects using the `pickle` module. This way, we can load them later in another script without the need to retrain the model or recalculate the molecular descriptors.\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "\n",
    "with open('path/to/model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# Load the model\n",
    "\n",
    "with open('path/to/model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb403d9e17203e7",
   "metadata": {},
   "source": [
    "---\n",
    "# Lab 4-2: Coding a simple app in Streamlit.\n",
    "\n",
    "Streamlit is a Python library that allows you to create very nice-looking and user-friendly web applications with just a few lines of code. It is very easy to use and requires absolutely no knowledge of HTML, CSS, or JavaScript.\n",
    "\n",
    "---\n",
    "\n",
    "We should have Streamlit already installed in our environment. You probably should take a look at the [Streamlit documentation](https://docs.streamlit.io/library), but this is how you can run a simple Streamlit app:\n",
    "\n",
    "Take look at `data/streamlit/app.py`. It is a tiny Streamlit app that takes a name as input and **returns a random fortune cookie-like quote**. Analyze the code and try to understand how it works. \n",
    "\n",
    "**Some important functions you may need to use:**\n",
    "\n",
    "- `st.title` sets the title of the app\n",
    "- `st.write` writes text to the app (supports [Markdown](https://www.markdownguide.org/basic-syntax/) synthax for text formatting)\n",
    "- `st.dataframe` displays a dataframe\n",
    "- `st.latex` renders LaTeX code (for mathematical formulas)\n",
    "- `st.text_input` creates a text input field (for multiline inputs, use `st.text_area`)\n",
    "- `st.button` creates a button (you can use it to trigger some action)\n",
    "\n",
    "To run the app, open a terminal and run the following command:\n",
    "\n",
    "    streamlit run data/streamlit/app.py\n",
    "    \n",
    "This will start a local server and open a new tab in your browser with the app. You can now interact with the app by entering your name and clicking the button to get a random fortune."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f89948b93e1fa0",
   "metadata": {},
   "source": [
    "## Exercise 5: Build an app for solubility prediction (3 points)\n",
    "\n",
    "This exercise is a continuation of the previous one. We will use all the code we have written so far to build a simple web app that predicts the solubility of molecules. The app will run in the browser and allow the user to input one or more SMILES strings, and it will display the predicted solubility of the molecules.\n",
    "The [Streamlit documentation](https://docs.streamlit.io/library) will be very helpful in this task.\n",
    "\n",
    "**The app should have the following structure:**\n",
    "\n",
    "1. A title and a short description of what the app does.\n",
    "2. A text area where the user can input a SMILES string (or multiple SMILES strings in a column).\n",
    "3. A button to submit the input.\n",
    "4. A section that displays the predicted solubility of the molecule(s) as a table of SMILES strings and their corresponding solubility values.\n",
    "\n",
    "The app should use our `Featurizer` class to extract the features from the input SMILES strings and predict the solubility using the trained model. **The model and the featurizer should not be trained inside the app, but loaded from a `.pkl` file instead!** Thanks to this approach, we will not need the training data to run the app. Both the model and the featurizer are already trained and saved as Python objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500173bbd37dfa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this exercise, you should create a new Python script called in the 'data/streamlit' directory and implement the app there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
