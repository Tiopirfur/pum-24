{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad10fe87c955413f",
   "metadata": {},
   "source": [
    "# Lab 6: Visualizing the training process of neural networks. Hyperparameter tuning.\n",
    "\n",
    "In this lab, you will learn how to use [wandb](https://wandb.ai/) to visualize the training process of neural networks. We are going to build and train a feed-forward neural network for recognizing handwritten digits of the MNIST dataset. The training process will be visualized in the wandb dashboard, which will allow us to monitor the loss and accuracy of the model in real-time.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Feel free to create an account at [wandb.ai](https://wandb.ai/) before starting this lab.\n",
    "\n",
    "### A simple example of how to use wandb in a typical training loop is shown below:\n",
    "\n",
    "```python\n",
    "import wandb\n",
    "\n",
    "wandb.login() # Log in to your wandb account\n",
    "\n",
    "# Start a new run\n",
    "\n",
    "some_config = {\n",
    "    'learning_rate': 0.01,\n",
    "    'layer_1_size': 128,\n",
    "    'layer_2_size': 64,\n",
    "    'batch_size': 32\n",
    "} # This is just an example of a configuration dictionary, you can put anything you want here\n",
    "\n",
    "wandb.init(project='mnist-classifier', config=some_config) # start a new run and log parameters\n",
    "\n",
    "# Here you would prepare your data, and initialize the model, optimizer, etc.\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    ...\n",
    "    wandb.log({'loss': loss, 'accuracy': accuracy})\n",
    "    # This will send the loss and accuracy to wandb and you can visualize it in the dashboard\n",
    "\n",
    "# End of the run\n",
    "wandb.finish()\n",
    "```\n",
    "\n",
    "The most important part is the `wandb.log()` function, which sends the data to the wandb dashboard. You can log any metric you want, not just loss and accuracy. The value passed to the function must be a dictionary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c701a2dc778ba9",
   "metadata": {},
   "source": [
    "## Exercise 1: Prepare data for training a mnist classifier (2 points)\n",
    "\n",
    "Before you start training a neural network, you need to prepare the data. In this exercise, you will prepare the MNIST dataset of handwritten digits for training a classifier. You should:\n",
    "\n",
    "1. Load the MNIST dataset using from `data/mnist_train.csv` and `data/mnist_test.csv` files.\n",
    "2. Normalize the data to the range [0, 1].\n",
    "3. Encode the labels using one-hot encoding.\n",
    "4. Create a PyTorch `Dataset` object for the training and test sets.\n",
    "5. Create a PyTorch `DataLoader` object for the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e73671250bc707ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0      1      2      3      4      5      6      7      8      9\n",
       "0      False  False  False  False  False   True  False  False  False  False\n",
       "1       True  False  False  False  False  False  False  False  False  False\n",
       "2      False  False  False  False   True  False  False  False  False  False\n",
       "3      False   True  False  False  False  False  False  False  False  False\n",
       "4      False  False  False  False  False  False  False  False  False   True\n",
       "...      ...    ...    ...    ...    ...    ...    ...    ...    ...    ...\n",
       "59995  False  False  False  False  False  False  False  False   True  False\n",
       "59996  False  False  False   True  False  False  False  False  False  False\n",
       "59997  False  False  False  False  False   True  False  False  False  False\n",
       "59998  False  False  False  False  False  False   True  False  False  False\n",
       "59999  False  False  False  False  False  False  False  False   True  False\n",
       "\n",
       "[60000 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code goes here\n",
    "import pandas as pd\n",
    "train = pd.read_csv('data/mnist-train.csv')\n",
    "test = pd.read_csv('data/mnist-test.csv')\n",
    "\n",
    "Y_train = train['label']\n",
    "X_train = train.drop('label', axis=1)\n",
    "\n",
    "Y_test = test['label']\n",
    "X_test = test.drop('label', axis=1)\n",
    "\n",
    "#print(X_train.shape, Y_train.shape)\n",
    "\n",
    "y_train = pd.get_dummies(Y_train)\n",
    "y_test = pd.get_dummies(Y_test)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5dc7e06a81f23f",
   "metadata": {},
   "source": [
    "## Exercise 2: Prepare the architecture of the neural network (2 points)\n",
    "\n",
    "In this exercise, you will prepare the architecture of the neural network. You should:\n",
    "\n",
    "1. Create a neural network class that inherits from `torch.nn.Module`.\n",
    "2. The neural network should have at least one hidden layer.\n",
    "3. Use ReLU activation functions after each but the output layer.\n",
    "4. Use a softmax activation function in the output layer to get the probabilities of each class.\n",
    "\n",
    "**Feel free to experiment with the architecture of your network** - try adding more hidden layers, changing the number of neurons in each layer, etc. You can also add a dropout layer or some other regularization technique and see if it improves the performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5310cb52194df1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X is a tensor of shape: torch.Size([32, 784])\n",
      "y is a tensor of shape: torch.Size([32, 10])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "\n",
      "tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "\n",
    "X_train_std = X_train/255\n",
    "X_test_std = X_test/255\n",
    "\n",
    "X_train_t = torch.tensor(X_train_std.to_numpy(), dtype=torch.float32)\n",
    "Y_train_t = torch.tensor(y_train.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "X_test_t = torch.tensor(X_test_std.to_numpy(), dtype=torch.float32)\n",
    "Y_test_t = torch.tensor(y_test.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "train = TensorDataset(X_train_t, Y_train_t)\n",
    "test = TensorDataset(X_test_t, Y_test_t)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=132, shuffle=False)\n",
    "\n",
    "X, y = next(iter(train_loader))\n",
    "\n",
    "print(\"X is a tensor of shape:\", X.shape)\n",
    "print(\"y is a tensor of shape:\", y.shape)\n",
    "print(X)\n",
    "print()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa3bb251-17a3-4b51-9686-acf78f5a832c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(X[0].reshape(28, 28), cmap='gray')\n",
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ef84220-6f27-4d19-b584-c95db32bd63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size_1, hidden_size_2):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, hidden_size_1)\n",
    "        self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
    "        self.fc3 = nn.Linear(hidden_size_2, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.fc1(x)\n",
    "        h = self.relu(h)\n",
    "        h = self.fc2(h)\n",
    "        h = self.relu(h)\n",
    "        h = self.fc3(h)\n",
    "        return self.output(h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e2efbd440294eb",
   "metadata": {},
   "source": [
    "## *Training PyTorch models on GPU\n",
    "\n",
    "**GPUs are optimized for performing matrix operations in parallel.** Although we call them \"graphics processing units\", they are actually very powerful processors that can be used for any kind of parallel computation, including training deep neural networks. In fact, data science is one of the most common applications of GPUs today, as can be seen by the revenue of companies like NVIDIA over the past few years. NVIDIA is a monopolist in the GPU market - in 2023, the company owned 92% of the data center GPU market share. As for 31 July, the 2024 revenue of NVIDIA was 60.92 billion USD, while the total revenue of 2020 was $10.92 billion. If someone benefits from the current deep learning hype, it is certainly NVIDIA.\n",
    "\n",
    "If you happen to have an NVIDIA GPU in your computer, you can use it to train your deep learning models, as PyTorch has excellent support for CUDA, which is NVIDIA's parallel computing API. To train a model on GPU, you need to explicitly tell PyTorch to move the model and the data to the GPU. \n",
    "\n",
    "Here is an example training loop that uses the GPU:\n",
    "\n",
    "```python\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # Check if a GPU is available\n",
    "\n",
    "# Initialize the model and move it to the GPU\n",
    "model = SomeNeuralNetwork().to(device)   # Move the model to the GPU\n",
    "\n",
    "for epoch in range(100):\n",
    "    for batch in data_loader:\n",
    "        X, y = batch\n",
    "        X, y = X.to(device), y.to(device)   # Move the tensors to GPU\n",
    "        \n",
    "        y_pred = model(X)   # Perform a forward pass (on the GPU)\n",
    "        loss = criterion(y_pred, y)   # Compute the loss (still on the GPU)\n",
    "        \n",
    "        ...  # The rest of the training loop\n",
    "        \n",
    "        y_pred = y_pred.detach().cpu()   # Move the predictions back to the CPU to do anything else with them\n",
    "```\n",
    "\n",
    "Note that **the model and all the tensors it uses for computation should be moved to the GPU**. You can do this by calling the `.to(device)` method on the model and the data tensors. If you want to move the data back to the CPU (to process it further, calculate metrics, visualize), you call the `.cpu()` method on the tensor.\n",
    "\n",
    "**Doing calculations on the GPU, you should be wary of few things:**\n",
    "\n",
    "* **The GPU has a limited amount of memory**, so you should be careful not to run out of memory. A typical graphics card has a few gigabytes of memory, so you should be fine with most models and datasets. However, moving very large tensors to the GPU can cause out-of-memory errors. That's one of the reasons why we use a dataloader and process the data in batches.\n",
    "* While the GPU is much faster than the CPU for large matrix operations, **transferring data between the CPU and the GPU is slow**. Therefore, it is best to minimize the number of data transfers between the CPU and the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94574b231d68216",
   "metadata": {},
   "source": [
    "## Exercise 3: Prepare the training loop (2 points)\n",
    "\n",
    "In this exercise, you will prepare the training loop. You should:\n",
    "\n",
    "1. Initialize the neural network.\n",
    "2. Define the loss function.\n",
    "3. Define the optimizer.\n",
    "4. Pass a dictionary with the configuration to wandb. This dictionary should contain all the hyperparameters of our model, including the learning rate, the size of the hidden layers, batch size, etc.\n",
    "4. Train the neural network. Each epoch should consist of a training and validation phase. You should log the loss and accuracy of the training and validation sets using wandb.\n",
    "5. Open you project at [wandb.ai](https://wandb.ai/) and see how cool it is!\n",
    "\n",
    "### Saving and loading the model\n",
    "As training can take some time, it is a good idea to save the model's state dictionary (its learned weights) to a file after training. You can do this with the following code:\n",
    "\n",
    "    torch.save(vae.state_dict(), 'vae.pth')\n",
    "    \n",
    "To load the model from the file, you can use the following code:\n",
    "\n",
    "    vae.load_state_dict(torch.load('vae.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d558333aa5fa0bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\admin\\_netrc\n",
      "wandb: Currently logged in as: tiopirfur (tiopirfur-uniwersytet-jagiello-ski-w-krakowie). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb584c1c3b504d90b6020c2ef74f118c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\admin\\Documents\\GitHub\\pum-24\\wandb\\run-20250127_123749-p6txmie9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tiopirfur-uniwersytet-jagiello-ski-w-krakowie/mnist-classifier/runs/p6txmie9' target=\"_blank\">solar-resonance-1</a></strong> to <a href='https://wandb.ai/tiopirfur-uniwersytet-jagiello-ski-w-krakowie/mnist-classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tiopirfur-uniwersytet-jagiello-ski-w-krakowie/mnist-classifier' target=\"_blank\">https://wandb.ai/tiopirfur-uniwersytet-jagiello-ski-w-krakowie/mnist-classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tiopirfur-uniwersytet-jagiello-ski-w-krakowie/mnist-classifier/runs/p6txmie9' target=\"_blank\">https://wandb.ai/tiopirfur-uniwersytet-jagiello-ski-w-krakowie/mnist-classifier/runs/p6txmie9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train loss: 1.6155208134969075\n",
      "Validation loss: 1.5371696729409068\n",
      "Epoch: 1\n",
      "Train loss: 1.523804834683736\n",
      "Validation loss: 1.5112780928611755\n",
      "Epoch: 2\n",
      "Train loss: 1.508758923403422\n",
      "Validation loss: 1.5025741881445835\n",
      "Epoch: 3\n",
      "Train loss: 1.4996286329269408\n",
      "Validation loss: 1.494748415131318\n",
      "Epoch: 4\n",
      "Train loss: 1.4943297377904257\n",
      "Validation loss: 1.4931037676961798\n",
      "Epoch: 5\n",
      "Train loss: 1.4893020144780478\n",
      "Validation loss: 1.4906071220573627\n",
      "Epoch: 6\n",
      "Train loss: 1.4865883273442586\n",
      "Validation loss: 1.4944061307530654\n",
      "Epoch: 7\n",
      "Train loss: 1.484776527150472\n",
      "Validation loss: 1.4917925298213959\n",
      "Epoch: 8\n",
      "Train loss: 1.4834686218897501\n",
      "Validation loss: 1.4905075195588564\n",
      "Epoch: 9\n",
      "Train loss: 1.4809174185434977\n",
      "Validation loss: 1.4895672798156738\n",
      "Epoch: 10\n",
      "Train loss: 1.4791995577494303\n",
      "Validation loss: 1.486901774218208\n",
      "Epoch: 11\n",
      "Train loss: 1.4786795873641967\n",
      "Validation loss: 1.4861769927175421\n",
      "Epoch: 12\n",
      "Train loss: 1.478247556622823\n",
      "Validation loss: 1.4915296058905751\n",
      "Epoch: 13\n",
      "Train loss: 1.4768560950597127\n",
      "Validation loss: 1.4864193106952466\n",
      "Epoch: 14\n",
      "Train loss: 1.4761573291778565\n",
      "Validation loss: 1.4903109340291274\n",
      "Epoch: 15\n",
      "Train loss: 1.4765824556350708\n",
      "Validation loss: 1.4851152959622835\n",
      "Epoch: 16\n",
      "Train loss: 1.4749840317408245\n",
      "Validation loss: 1.4845702820702602\n",
      "Epoch: 17\n",
      "Train loss: 1.474802306175232\n",
      "Validation loss: 1.4859918983359086\n",
      "Epoch: 18\n",
      "Train loss: 1.474208460299174\n",
      "Validation loss: 1.4853015212636245\n",
      "Epoch: 19\n",
      "Train loss: 1.4737321095784506\n",
      "Validation loss: 1.4839582992227454\n",
      "Epoch: 20\n",
      "Train loss: 1.4746637010574342\n",
      "Validation loss: 1.492465339208904\n",
      "Epoch: 21\n",
      "Train loss: 1.4738458079655965\n",
      "Validation loss: 1.4860822244694358\n",
      "Epoch: 22\n",
      "Train loss: 1.4730457051595052\n",
      "Validation loss: 1.4848206482435529\n",
      "Epoch: 23\n",
      "Train loss: 1.4726902186075845\n",
      "Validation loss: 1.4828871943448718\n",
      "Epoch: 24\n",
      "Train loss: 1.472543444887797\n",
      "Validation loss: 1.4839781758032347\n",
      "Epoch: 25\n",
      "Train loss: 1.4721424617767334\n",
      "Validation loss: 1.4842248449200077\n",
      "Epoch: 26\n",
      "Train loss: 1.4715415114720662\n",
      "Validation loss: 1.483603846085699\n",
      "Epoch: 27\n",
      "Train loss: 1.4721242281595865\n",
      "Validation loss: 1.4858144725623883\n",
      "Epoch: 28\n",
      "Train loss: 1.4724329160054526\n",
      "Validation loss: 1.4836191378141705\n",
      "Epoch: 29\n",
      "Train loss: 1.4705563833872477\n",
      "Validation loss: 1.4841853756653636\n",
      "Epoch: 30\n",
      "Train loss: 1.4713317916234334\n",
      "Validation loss: 1.4829668590897007\n",
      "Epoch: 31\n",
      "Train loss: 1.4718130755742391\n",
      "Validation loss: 1.4826160136022066\n",
      "Epoch: 32\n",
      "Train loss: 1.4705208731969197\n",
      "Validation loss: 1.4840835379926782\n",
      "Epoch: 33\n",
      "Train loss: 1.4699081259409588\n",
      "Validation loss: 1.4870455437584926\n",
      "Epoch: 34\n",
      "Train loss: 1.4719397190729777\n",
      "Validation loss: 1.4833300521499233\n",
      "Epoch: 35\n",
      "Train loss: 1.4701930880228677\n",
      "Validation loss: 1.4896232705367238\n",
      "Epoch: 36\n",
      "Train loss: 1.4707687966664633\n",
      "Validation loss: 1.4847404580367238\n",
      "Epoch: 37\n",
      "Train loss: 1.471290402285258\n",
      "Validation loss: 1.4860598507680391\n",
      "Epoch: 38\n",
      "Train loss: 1.4708025953292847\n",
      "Validation loss: 1.4822434626127545\n",
      "Epoch: 39\n",
      "Train loss: 1.4703213143666585\n",
      "Validation loss: 1.4869503520037\n",
      "Epoch: 40\n",
      "Train loss: 1.4704778376897176\n",
      "Validation loss: 1.4855281585141231\n",
      "Epoch: 41\n",
      "Train loss: 1.4693701955159506\n",
      "Validation loss: 1.483615014113878\n",
      "Epoch: 42\n",
      "Train loss: 1.470831289100647\n",
      "Validation loss: 1.4825462714621895\n",
      "Epoch: 43\n",
      "Train loss: 1.4707432015736899\n",
      "Validation loss: 1.4831605788908506\n",
      "Epoch: 44\n",
      "Train loss: 1.4695471777598064\n",
      "Validation loss: 1.482614451333096\n",
      "Epoch: 45\n",
      "Train loss: 1.4696769832611083\n",
      "Validation loss: 1.482970217340871\n",
      "Epoch: 46\n",
      "Train loss: 1.4700873273213704\n",
      "Validation loss: 1.4838167883847888\n",
      "Epoch: 47\n",
      "Train loss: 1.4700070479710896\n",
      "Validation loss: 1.4831221605602063\n",
      "Epoch: 48\n",
      "Train loss: 1.4698166134516397\n",
      "Validation loss: 1.4843653111081374\n",
      "Epoch: 49\n",
      "Train loss: 1.470591519355774\n",
      "Validation loss: 1.4827067397142712\n",
      "Epoch: 50\n",
      "Train loss: 1.4696741027196247\n",
      "Validation loss: 1.4836795000653518\n",
      "Epoch: 51\n",
      "Train loss: 1.4697306783676147\n",
      "Validation loss: 1.4852557511706102\n",
      "Epoch: 52\n",
      "Train loss: 1.469025891049703\n",
      "Validation loss: 1.483956559708244\n",
      "Epoch: 53\n",
      "Train loss: 1.4696081745147704\n",
      "Validation loss: 1.4824981642396826\n",
      "Epoch: 54\n",
      "Train loss: 1.4697495665868123\n",
      "Validation loss: 1.4842238081128973\n",
      "Epoch: 55\n",
      "Train loss: 1.4698793596903483\n",
      "Validation loss: 1.4848132243281917\n",
      "Epoch: 56\n",
      "Train loss: 1.4698339935938518\n",
      "Validation loss: 1.48698406313595\n",
      "Epoch: 57\n",
      "Train loss: 1.4696290934244791\n",
      "Validation loss: 1.4825348556041718\n",
      "Epoch: 58\n",
      "Train loss: 1.4693445846557618\n",
      "Validation loss: 1.4885171714581942\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m     46\u001b[0m model \u001b[38;5;241m=\u001b[39m NeuralNetwork(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, epochs)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:   \u001b[38;5;66;03m# load data batch-by-batch; trzeba dzieliÄ‡ na batche, bo karty graficzne majÄ… ograniczonÄ… pamiÄ™Ä‡ VRAM.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m#JakbyÅ›my nie mieli batche, to gradienty po caÅ‚ym datasecie byÅ‚yby zawsze takie same. Przez to, Å¼e mamy rÃ³Å¼ne dane w batchach to spadek jest losowy\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m#(stochastyczny gradient) i moÅ¼e wyjdziemy z lokalnych minimÃ³w.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()   \u001b[38;5;66;03m# clear the gradients\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     batch_loss \u001b[38;5;241m=\u001b[39m loss(y_pred, y_batch) \u001b[38;5;66;03m# compute the loss\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     batch_loss\u001b[38;5;241m.\u001b[39mbackward()   \u001b[38;5;66;03m# compute the gradients\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pum\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[16], line 16\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(h)\n\u001b[0;32m     15\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(h)\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pum\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pum\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1390\u001b[0m, in \u001b[0;36mSoftmax.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pum\\lib\\site-packages\\torch\\nn\\functional.py:1841\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1839\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1841\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1842\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1843\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "import wandb\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs=100):\n",
    "\n",
    "    wandb.login(key='876dfec77c2e4a68b24ec9499a55665f94652f25')\n",
    "    wandb.init(project='mnist-classifier', config={'learning_rate': 0.01, 'layer_1_size': 128, 'layer_2_size': 64, 'batch_size': 32})\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()   # set the model to training mode (some layers may behave differently in training and evaluation)\n",
    "        train_loss = 0  # this variable will accumulate the training loss\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:   # load data batch-by-batch; trzeba dzieliÄ‡ na batche, bo karty graficzne majÄ… ograniczonÄ… pamiÄ™Ä‡ VRAM.\n",
    "            #JakbyÅ›my nie mieli batche, to gradienty po caÅ‚ym datasecie byÅ‚yby zawsze takie same. Przez to, Å¼e mamy rÃ³Å¼ne dane w batchach to spadek jest losowy\n",
    "            #(stochastyczny gradient) i moÅ¼e wyjdziemy z lokalnych minimÃ³w.\n",
    "            \n",
    "            optimizer.zero_grad()   # clear the gradients\n",
    "            y_pred = model(X_batch)  # forward pass\n",
    "            batch_loss = loss(y_pred, y_batch) # compute the loss\n",
    "            batch_loss.backward()   # compute the gradients\n",
    "            optimizer.step()    # update the weights\n",
    "            \n",
    "            train_loss += batch_loss.item() # accumulate training loss\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader) # compute the average loss\n",
    "        print(f'Epoch: {epoch}')\n",
    "        print(f'Train loss: {train_loss}')\n",
    "                  \n",
    "        model.eval()    # set the model to evaluation mode\n",
    "        val_loss = 0    # this variable will accumulate the validation loss\n",
    "        \n",
    "        for X_batch, y_batch in val_loader:\n",
    "            \n",
    "            y_pred = model(X_batch)\n",
    "            val_loss += loss(y_pred, y_batch).item()    # accumulate validation loss\n",
    "            \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        print(f'Validation loss: {val_loss}')\n",
    "        wandb.log({'train_loss': train_loss, 'val_loss': val_loss})\n",
    "        \n",
    "    return model\n",
    "\n",
    "model = NeuralNetwork(128, 64)\n",
    "train(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77990efcffab975",
   "metadata": {},
   "source": [
    "## Exercise 4: Easy hyperparameter tuning with wandb (2 points)\n",
    "\n",
    "Wandb allows you to perform hyperparameter tuning by automatically creating multiple runs with different hyperparameters and logging the performance of each run. Below is a brief instruction to `wandb` hyperparameter tuning, but you are more than welcome to find more information in the [official wandb guide](https://docs.wandb.ai/guides/sweeps/).\n",
    "\n",
    "Your task is to use wandb to perform hyperparameter tuning of the neural network, trying different values of the learning rate, batch size, and the size of the hidden layers. You can use the following hyperparameters:\n",
    "\n",
    "First, we need to define a dictionary with the hyperparameters that we want to tune. For example:\n",
    "\n",
    "```python\n",
    "parameters = {\n",
    "    'learning_rate': {'values': [0.01, 0.001, 0.0001]},\n",
    "    'batch_size': {'values': [32, 64, 128]},\n",
    "    'layer_1_size': {'values': [64, 128, 256]},\n",
    "    'layer_2_size': {'values': [32, 64, 128]}\n",
    "}\n",
    "```\n",
    "\n",
    "Then we need to create a dictionary with the configuration of the run:\n",
    "\n",
    "```python\n",
    "sweep_config = {\n",
    "    'name': 'mnist-sweep',\n",
    "    'method': 'grid',   # grid search, you can also try 'random' or 'bayes'\n",
    "    'metric': {'goal': 'minimize', 'name': 'val_loss'},\n",
    "    'parameters': parameters,   # that's the dictionary with the hyperparameters\n",
    "}\n",
    "```\n",
    "\n",
    "Finally, we can use the `wandb.sweep` function to perform hyperparameter tuning:\n",
    "\n",
    "```python\n",
    "sweep_id = wandb.sweep(sweep_config, project='mnist-classifier')\n",
    "```\n",
    "\n",
    "After that, we can finally run the sweep:\n",
    "\n",
    "```python\n",
    "wandb.agent(sweep_id, function=train)\n",
    "```\n",
    "where `train` is a function that trains the model and logs the metrics to wandb. This function should take a `config` argument, which will contain the hyperparameters of the run. That is how wandb knows which hyperparameters to tune.\n",
    "\n",
    "1. Rewrite the VAE training loop into a function that takes a single dictionary `parameters` as an argument, initializes the model, optimizer, and criterion, and trains the model for a fixed number of epochs. The function should log the loss and accuracy of the training and validation sets to wandb.\n",
    "2. Create a dictionary with the hyperparameters that you want to tune.\n",
    "3. Create a sweep configuration dictionary.\n",
    "4. Run the sweep and monitor the results in the wandb dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35f63fb35d55201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "def train(parameters: dict):\n",
    "    # your code goes here\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b896fac35c516b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {...}\n",
    "\n",
    "sweep_config = {\n",
    "    'name': 'mnist-sweep',\n",
    "    'method': 'bayes',\n",
    "    'metric': {'goal': 'maximize', 'name': 'accuracy'}, # if we want to maximize the accuracy\n",
    "    # remember to log the metric that you want to maximize or minimize!\n",
    "    'parameters': parameters,\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project='mnist-classifier')    # This will create a new sweep\n",
    "wandb.agent(sweep_id, function=train)   # This will start the hyperparameter tuning process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
